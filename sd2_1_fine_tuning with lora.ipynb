{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe290BSnzsWM",
        "outputId": "59029677-42da-403a-faa4-9924347fed93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "--2025-04-24 19:09:51--  https://raw.githubusercontent.com/huggingface/diffusers/refs/heads/main/examples/text_to_image/train_text_to_image_lora.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40568 (40K) [text/plain]\n",
            "Saving to: ‘train_text_to_image_lora.py’\n",
            "\n",
            "train_text_to_image 100%[===================>]  39.62K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-24 19:09:51 (34.9 MB/s) - ‘train_text_to_image_lora.py’ saved [40568/40568]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for SD 2.1 and Diffusers with LoRA support\n",
        "!pip install bitsandbytes transformers accelerate peft datasets -q\n",
        "!pip install git+https://github.com/huggingface/diffusers.git -q\n",
        "# Download training script for SD 2.1 (with LoRA support)\n",
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/refs/heads/main/examples/text_to_image/train_text_to_image_lora.py -O train_text_to_image_lora.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47kpJpHQ_P39",
        "outputId": "f331d0f2-b92d-4caf-b1ef-e1d5f222a6eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from xformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->xformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6LkdFTQzy6b"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "notebook_key=userdata.get('notebooks')\n",
        "login(notebook_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4-lQ7-8zwXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafd2d28-75fc-44de-a48e-4fad18a80f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-24 19:10:00.608244: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-24 19:10:00.625743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745521800.646742   10120 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745521800.653163   10120 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-24 19:10:00.674592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "04/24/2025 19:10:16 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "{'variance_type', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio', 'timestep_spacing', 'sample_max_value', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "{'class_embed_type', 'conv_out_kernel', 'class_embeddings_concat', 'mid_block_type', 'mid_block_only_cross_attention', 'cross_attention_norm', 'projection_class_embeddings_input_dim', 'time_embedding_act_fn', 'resnet_skip_time_act', 'timestep_post_act', 'time_cond_proj_dim', 'encoder_hid_dim', 'num_attention_heads', 'time_embedding_type', 'reverse_transformer_layers_per_block', 'time_embedding_dim', 'conv_in_kernel', 'dropout', 'transformer_layers_per_block', 'upcast_attention', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'encoder_hid_dim_type', 'resnet_out_scale_factor', 'attention_type', 'addition_embed_type', 'resnet_time_scale_shift'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "04/24/2025 19:10:26 - INFO - __main__ - ***** Running training *****\n",
            "04/24/2025 19:10:26 - INFO - __main__ -   Num examples = 10000\n",
            "04/24/2025 19:10:26 - INFO - __main__ -   Num Epochs = 20\n",
            "04/24/2025 19:10:26 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
            "04/24/2025 19:10:26 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "04/24/2025 19:10:26 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
            "04/24/2025 19:10:26 - INFO - __main__ -   Total optimization steps = 6000\n",
            "Steps:   5% 313/6000 [17:52<4:33:20,  2.88s/it, lr=0.000125, step_loss=0.0346]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.89it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.30it/s]\n",
            "04/24/2025 19:28:19 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:   8% 500/6000 [28:48<5:14:07,  3.43s/it, lr=0.0002, step_loss=0.104]04/24/2025 19:39:14 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-500\n",
            "04/24/2025 19:39:29 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-500/model.safetensors\n",
            "04/24/2025 19:39:29 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-500/optimizer.bin\n",
            "04/24/2025 19:39:29 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-500/scheduler.bin\n",
            "04/24/2025 19:39:29 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-500/sampler.bin\n",
            "04/24/2025 19:39:29 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-500/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-500/pytorch_lora_weights.safetensors\n",
            "04/24/2025 19:39:29 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-500\n",
            "Steps:  16% 939/6000 [54:04<4:03:02,  2.88s/it, lr=0.000197, step_loss=0.0355]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.58it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 18.89it/s]\n",
            "04/24/2025 20:04:32 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:  17% 1000/6000 [57:47<4:44:21,  3.41s/it, lr=0.000196, step_loss=0.0821]04/24/2025 20:08:14 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-1000\n",
            "04/24/2025 20:08:28 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-1000/model.safetensors\n",
            "04/24/2025 20:08:28 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-1000/optimizer.bin\n",
            "04/24/2025 20:08:28 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-1000/scheduler.bin\n",
            "04/24/2025 20:08:28 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-1000/sampler.bin\n",
            "04/24/2025 20:08:28 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-1000/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-1000/pytorch_lora_weights.safetensors\n",
            "04/24/2025 20:08:28 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-1000\n",
            "Steps:  25% 1500/6000 [1:26:34<4:17:32,  3.43s/it, lr=0.000184, step_loss=0.0168]04/24/2025 20:37:01 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-1500\n",
            "04/24/2025 20:37:16 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-1500/model.safetensors\n",
            "04/24/2025 20:37:16 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-1500/optimizer.bin\n",
            "04/24/2025 20:37:16 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-1500/scheduler.bin\n",
            "04/24/2025 20:37:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-1500/sampler.bin\n",
            "04/24/2025 20:37:16 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-1500/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-1500/pytorch_lora_weights.safetensors\n",
            "04/24/2025 20:37:16 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-1500\n",
            "Steps:  26% 1565/6000 [1:30:31<3:32:29,  2.87s/it, lr=0.000182, step_loss=0.0417]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.74it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.30it/s]\n",
            "04/24/2025 20:40:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:  33% 2000/6000 [1:55:33<3:48:27,  3.43s/it, lr=0.000166, step_loss=0.0732]04/24/2025 21:06:00 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-2000\n",
            "04/24/2025 21:06:10 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-2000/model.safetensors\n",
            "04/24/2025 21:06:10 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-2000/optimizer.bin\n",
            "04/24/2025 21:06:10 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-2000/scheduler.bin\n",
            "04/24/2025 21:06:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-2000/sampler.bin\n",
            "04/24/2025 21:06:10 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-2000/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-2000/pytorch_lora_weights.safetensors\n",
            "04/24/2025 21:06:10 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-2000\n",
            "Steps:  37% 2191/6000 [2:06:36<3:02:13,  2.87s/it, lr=0.000157, step_loss=0.081]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.90it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.40it/s]\n",
            "04/24/2025 21:17:03 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:  42% 2500/6000 [2:24:29<3:19:53,  3.43s/it, lr=0.000142, step_loss=0.0662]04/24/2025 21:34:55 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-2500\n",
            "04/24/2025 21:35:09 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-2500/model.safetensors\n",
            "04/24/2025 21:35:10 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-2500/optimizer.bin\n",
            "04/24/2025 21:35:10 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-2500/scheduler.bin\n",
            "04/24/2025 21:35:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-2500/sampler.bin\n",
            "04/24/2025 21:35:10 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-2500/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-2500/pytorch_lora_weights.safetensors\n",
            "04/24/2025 21:35:10 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-2500\n",
            "Steps:  47% 2817/6000 [2:42:46<2:32:55,  2.88s/it, lr=0.000124, step_loss=0.105]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.63it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.20it/s]\n",
            "04/24/2025 21:53:13 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:  50% 3000/6000 [2:53:28<2:51:27,  3.43s/it, lr=0.000114, step_loss=0.044]04/24/2025 22:03:54 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-3000\n",
            "04/24/2025 22:04:03 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-3000/model.safetensors\n",
            "04/24/2025 22:04:03 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-3000/optimizer.bin\n",
            "04/24/2025 22:04:03 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-3000/scheduler.bin\n",
            "04/24/2025 22:04:03 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-3000/sampler.bin\n",
            "04/24/2025 22:04:03 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-3000/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-3000/pytorch_lora_weights.safetensors\n",
            "04/24/2025 22:04:04 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-3000\n",
            "Steps:  57% 3443/6000 [3:18:51<2:02:01,  2.86s/it, lr=8.9e-5, step_loss=0.0691]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.75it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.39it/s]\n",
            "04/24/2025 22:29:19 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:  58% 3500/6000 [3:22:20<2:22:31,  3.42s/it, lr=8.58e-5, step_loss=0.134]04/24/2025 22:32:47 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-3500\n",
            "04/24/2025 22:33:00 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-3500/model.safetensors\n",
            "04/24/2025 22:33:00 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-3500/optimizer.bin\n",
            "04/24/2025 22:33:00 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-3500/scheduler.bin\n",
            "04/24/2025 22:33:00 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-3500/sampler.bin\n",
            "04/24/2025 22:33:00 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-3500/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-3500/pytorch_lora_weights.safetensors\n",
            "04/24/2025 22:33:00 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-3500\n",
            "Steps:  67% 4000/6000 [3:51:04<1:54:08,  3.42s/it, lr=5.85e-5, step_loss=0.042]04/24/2025 23:01:30 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-4000\n",
            "04/24/2025 23:01:44 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-4000/model.safetensors\n",
            "04/24/2025 23:01:44 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-4000/optimizer.bin\n",
            "04/24/2025 23:01:44 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-4000/scheduler.bin\n",
            "04/24/2025 23:01:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-4000/sampler.bin\n",
            "04/24/2025 23:01:44 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-4000/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-4000/pytorch_lora_weights.safetensors\n",
            "04/24/2025 23:01:44 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-4000\n",
            "Steps:  68% 4069/6000 [3:55:13<1:33:03,  2.89s/it, lr=5.49e-5, step_loss=0.268]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.68it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.31it/s]\n",
            "04/24/2025 23:05:40 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:  75% 4500/6000 [4:20:03<1:25:24,  3.42s/it, lr=3.46e-5, step_loss=0.0982]04/24/2025 23:30:30 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-4500\n",
            "04/24/2025 23:30:41 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-4500/model.safetensors\n",
            "04/24/2025 23:30:41 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-4500/optimizer.bin\n",
            "04/24/2025 23:30:41 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-4500/scheduler.bin\n",
            "04/24/2025 23:30:41 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-4500/sampler.bin\n",
            "04/24/2025 23:30:41 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-4500/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-4500/pytorch_lora_weights.safetensors\n",
            "04/24/2025 23:30:41 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-4500\n",
            "Steps:  78% 4695/6000 [4:31:21<1:02:15,  2.86s/it, lr=2.65e-5, step_loss=0.00619]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.83it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.52it/s]\n",
            "04/24/2025 23:41:48 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:  83% 5000/6000 [4:49:01<57:08,  3.43s/it, lr=1.59e-5, step_loss=0.047]04/24/2025 23:59:27 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-5000\n",
            "04/24/2025 23:59:40 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-5000/model.safetensors\n",
            "04/24/2025 23:59:40 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-5000/optimizer.bin\n",
            "04/24/2025 23:59:40 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-5000/scheduler.bin\n",
            "04/24/2025 23:59:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-5000/sampler.bin\n",
            "04/24/2025 23:59:40 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-5000/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-5000/pytorch_lora_weights.safetensors\n",
            "04/24/2025 23:59:40 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-5000\n",
            "Steps:  89% 5321/6000 [5:07:32<32:37,  2.88s/it, lr=7.43e-6, step_loss=0.0995]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.83it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.45it/s]\n",
            "04/25/2025 00:17:59 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps:  92% 5500/6000 [5:17:59<28:37,  3.43s/it, lr=4.07e-6, step_loss=0.0989]04/25/2025 00:28:25 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-5500\n",
            "04/25/2025 00:28:40 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-5500/model.safetensors\n",
            "04/25/2025 00:28:40 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-5500/optimizer.bin\n",
            "04/25/2025 00:28:40 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-5500/scheduler.bin\n",
            "04/25/2025 00:28:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-5500/sampler.bin\n",
            "04/25/2025 00:28:40 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-5500/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-5500/pytorch_lora_weights.safetensors\n",
            "04/25/2025 00:28:40 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-5500\n",
            "Steps:  99% 5947/6000 [5:43:40<02:33,  2.90s/it, lr=4.58e-8, step_loss=0.194] {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  9.65it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 19.19it/s]\n",
            "04/25/2025 00:54:07 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps: 100% 6000/6000 [5:46:56<00:00,  3.43s/it, lr=1.63e-11, step_loss=0.0881]04/25/2025 00:57:23 - INFO - accelerate.accelerator - Saving current state to sd21-hm-fashion-lora/checkpoint-6000\n",
            "04/25/2025 00:57:37 - INFO - accelerate.checkpointing - Model weights saved in sd21-hm-fashion-lora/checkpoint-6000/model.safetensors\n",
            "04/25/2025 00:57:37 - INFO - accelerate.checkpointing - Optimizer state saved in sd21-hm-fashion-lora/checkpoint-6000/optimizer.bin\n",
            "04/25/2025 00:57:37 - INFO - accelerate.checkpointing - Scheduler state saved in sd21-hm-fashion-lora/checkpoint-6000/scheduler.bin\n",
            "04/25/2025 00:57:37 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in sd21-hm-fashion-lora/checkpoint-6000/sampler.bin\n",
            "04/25/2025 00:57:37 - INFO - accelerate.checkpointing - Random states saved in sd21-hm-fashion-lora/checkpoint-6000/random_states_0.pkl\n",
            "Model weights saved in sd21-hm-fashion-lora/checkpoint-6000/pytorch_lora_weights.safetensors\n",
            "04/25/2025 00:57:37 - INFO - __main__ - Saved state to sd21-hm-fashion-lora/checkpoint-6000\n",
            "Steps: 100% 6000/6000 [5:47:11<00:00,  3.43s/it, lr=0, step_loss=0.0593]       Model weights saved in sd21-hm-fashion-lora/pytorch_lora_weights.safetensors\n",
            "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float32.\n",
            "{'class_embed_type', 'conv_out_kernel', 'class_embeddings_concat', 'mid_block_type', 'mid_block_only_cross_attention', 'cross_attention_norm', 'projection_class_embeddings_input_dim', 'time_embedding_act_fn', 'resnet_skip_time_act', 'timestep_post_act', 'time_cond_proj_dim', 'encoder_hid_dim', 'num_attention_heads', 'time_embedding_type', 'reverse_transformer_layers_per_block', 'time_embedding_dim', 'conv_in_kernel', 'dropout', 'transformer_layers_per_block', 'upcast_attention', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'encoder_hid_dim_type', 'resnet_out_scale_factor', 'attention_type', 'addition_embed_type', 'resnet_time_scale_shift'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/unet.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  17% 1/6 [00:00<00:00,  8.31it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00,  5.76it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'scaling_factor', 'force_upcast', 'latents_mean', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
            "\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 13.75it/s]\n",
            "Loading unet.\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "04/25/2025 00:57:39 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: a red dress on plain white background, product photo.\n",
            "Steps: 100% 6000/6000 [5:47:25<00:00,  3.47s/it, lr=0, step_loss=0.0593]\n"
          ]
        }
      ],
      "source": [
        "!python train_text_to_image_lora.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1-base\" \\\n",
        "  --dataset_name=\"tomytjandra/h-and-m-fashion-caption-12k\" \\\n",
        "  --caption_column=\"text\" \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=4 \\\n",
        "  --gradient_accumulation_steps=8 \\\n",
        "  --lr_scheduler=\"cosine_with_restarts\" \\\n",
        "  --lr_warmup_steps=500 \\\n",
        "  --learning_rate=2e-4 \\\n",
        "  --max_train_steps=6000 \\\n",
        "  --checkpointing_steps=500 \\\n",
        "  --max_train_samples=10000 \\\n",
        "  --validation_prompt=\"a red dress on plain white background, product photo\" \\\n",
        "  --validation_epochs=2 \\\n",
        "  --seed=42 \\\n",
        "  --output_dir=\"sd21-hm-fashion-lora\" \\\n",
        "  --num_train_epochs=100 \\\n",
        "  --enable_xformers_memory_efficient_attention \\\n",
        "  --rank=16 \\\n",
        "  --allow_tf32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtWqHo6-_3uH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "8d7952161ede4626887c54954d2cf5d2",
            "44ad9f6b4e8d4801a5715bb96423fc0d",
            "eb775f1eba5a4b919f5253af594c1f27",
            "9259af4b018b4f5aafc7eae23750a178",
            "cd3039ed9c2d4993a3952178ab361e49",
            "8d045a2663f74087ad2aac3a0c0824ec",
            "700f1c9047114959903039f6c7978243",
            "ed18e433c2ec40b28993938ddfaca31d",
            "c6f24bbbc0534d8bab9a8fb256ef69b6",
            "f7e0883f910e410fbc8d5cabaaa9a475",
            "a9155225273a41c88deb714c1e5da1d4",
            "c09b7eaabf55495aa19fb4c6ca63bbd0",
            "3b1da6ba30ac448c8895d0cdc14dd3e6",
            "8683854724e24c53800a03b3d59d5162",
            "469a1bdccffd4e4dad2f6fa004a952e6",
            "563f7da4392a4dd9987b37818e3e7d36",
            "905a3bafd6fd4c4fbc83fa7be123cd97",
            "00493b18fe924f84b8a1c7162ec7c00b",
            "dba2bc5759344848a10204700b8c9381",
            "84ec2da82aed4fbb8a92b1e3812d7aff",
            "511cb549d2154273a38b406ba1ee7a54",
            "f62e70bf3e1148df956636cbbc1ecae7"
          ]
        },
        "outputId": "44b86ba3-a8da-4c0a-ad18-5654d425fea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d7952161ede4626887c54954d2cf5d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/diffusers/loaders/unet.py:212: FutureWarning: `load_attn_procs` is deprecated and will be removed in version 0.40.0. Using the `load_attn_procs()` method has been deprecated and will be removed in a future version. Please use `load_lora_adapter()`.\n",
            "  deprecate(\"load_attn_procs\", \"0.40.0\", deprecation_message)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c09b7eaabf55495aa19fb4c6ca63bbd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated with prompt: 'simple purple check tshirt, on plain white background, isolated product photo'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers.loaders import AttnProcsLayers\n",
        "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
        "\n",
        "def inference_lora_sd(\n",
        "    prompt: str,\n",
        "    lora_model_path: str,\n",
        "    negative_prompt: str = \"low quality, bad quality, blurry\",\n",
        "    num_images: int = 1,\n",
        "    num_inference_steps: int = 30,\n",
        "    guidance_scale: float = 7.5,\n",
        "    seed: int = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate images using a LoRA fine-tuned Stable Diffusion 2.1 model.\n",
        "    Automatically adds \"on plain white background\" to every prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: Text prompt for image generation (will be enhanced)\n",
        "        lora_model_path: Path to the fine-tuned LoRA weights file\n",
        "        negative_prompt: Negative prompt to guide generation away from undesired results\n",
        "        num_images: Number of images to generate\n",
        "        num_inference_steps: Number of denoising steps (more = higher quality, slower)\n",
        "        guidance_scale: How closely to follow the prompt (higher = more faithful but less diverse)\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        List of generated images\n",
        "    \"\"\"\n",
        "    # Enhance prompt with plain white background specification\n",
        "    enhanced_prompt = f\"{prompt}, on plain white background, isolated product photo\"\n",
        "\n",
        "    # Set random seed if provided\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Load base model\n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-2-1-base\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Load LoRA weights\n",
        "    pipeline.unet.load_attn_procs(lora_model_path)\n",
        "\n",
        "    # Generate images\n",
        "    images = pipeline(\n",
        "        prompt=enhanced_prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_images,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale\n",
        "    ).images\n",
        "\n",
        "    print(f\"Generated with prompt: '{enhanced_prompt}'\")\n",
        "    return images\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    image = inference_lora_sd(\n",
        "        prompt=\"simple purple check tshirt\",\n",
        "        lora_model_path=\"./sd21-hm-fashion-lora\",  # Path to your trained LoRA weights\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    image[0].save(f\"fashion_item.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive('/content/sd21-hm-fashion-lora', 'zip', '/content/sd21-hm-fashion-lora')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tRiD8AYZd7yj",
        "outputId": "096baa1f-283b-4602-ee2e-356ef8292aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/sd21-hm-fashion-lora.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/sd21-hm-fashion-lora.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iasL2SI_eB3a",
        "outputId": "f7a95e1d-d4de-415c-c92d-445f4d82de4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80b1e009-591b-4645-8100-ca8f1494ef86\", \"sd21-hm-fashion-lora.zip\", 39248456033)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d7952161ede4626887c54954d2cf5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44ad9f6b4e8d4801a5715bb96423fc0d",
              "IPY_MODEL_eb775f1eba5a4b919f5253af594c1f27",
              "IPY_MODEL_9259af4b018b4f5aafc7eae23750a178"
            ],
            "layout": "IPY_MODEL_cd3039ed9c2d4993a3952178ab361e49"
          }
        },
        "44ad9f6b4e8d4801a5715bb96423fc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d045a2663f74087ad2aac3a0c0824ec",
            "placeholder": "​",
            "style": "IPY_MODEL_700f1c9047114959903039f6c7978243",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "eb775f1eba5a4b919f5253af594c1f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed18e433c2ec40b28993938ddfaca31d",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6f24bbbc0534d8bab9a8fb256ef69b6",
            "value": 6
          }
        },
        "9259af4b018b4f5aafc7eae23750a178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7e0883f910e410fbc8d5cabaaa9a475",
            "placeholder": "​",
            "style": "IPY_MODEL_a9155225273a41c88deb714c1e5da1d4",
            "value": " 6/6 [00:01&lt;00:00,  4.08it/s]"
          }
        },
        "cd3039ed9c2d4993a3952178ab361e49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d045a2663f74087ad2aac3a0c0824ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "700f1c9047114959903039f6c7978243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed18e433c2ec40b28993938ddfaca31d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6f24bbbc0534d8bab9a8fb256ef69b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7e0883f910e410fbc8d5cabaaa9a475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9155225273a41c88deb714c1e5da1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c09b7eaabf55495aa19fb4c6ca63bbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b1da6ba30ac448c8895d0cdc14dd3e6",
              "IPY_MODEL_8683854724e24c53800a03b3d59d5162",
              "IPY_MODEL_469a1bdccffd4e4dad2f6fa004a952e6"
            ],
            "layout": "IPY_MODEL_563f7da4392a4dd9987b37818e3e7d36"
          }
        },
        "3b1da6ba30ac448c8895d0cdc14dd3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_905a3bafd6fd4c4fbc83fa7be123cd97",
            "placeholder": "​",
            "style": "IPY_MODEL_00493b18fe924f84b8a1c7162ec7c00b",
            "value": "100%"
          }
        },
        "8683854724e24c53800a03b3d59d5162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba2bc5759344848a10204700b8c9381",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84ec2da82aed4fbb8a92b1e3812d7aff",
            "value": 30
          }
        },
        "469a1bdccffd4e4dad2f6fa004a952e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_511cb549d2154273a38b406ba1ee7a54",
            "placeholder": "​",
            "style": "IPY_MODEL_f62e70bf3e1148df956636cbbc1ecae7",
            "value": " 30/30 [00:02&lt;00:00, 15.57it/s]"
          }
        },
        "563f7da4392a4dd9987b37818e3e7d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "905a3bafd6fd4c4fbc83fa7be123cd97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00493b18fe924f84b8a1c7162ec7c00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dba2bc5759344848a10204700b8c9381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84ec2da82aed4fbb8a92b1e3812d7aff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "511cb549d2154273a38b406ba1ee7a54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f62e70bf3e1148df956636cbbc1ecae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}