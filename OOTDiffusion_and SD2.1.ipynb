{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Creating envs\n",
        "\n",
        "1- sdxl env for the generation model\n",
        "\n",
        "2-ootd env for the virtual try on model"
      ],
      "metadata": {
        "id": "0w_LGB-ST5L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install virtualenv if not available\n",
        "!pip install virtualenv -q\n",
        "\n",
        "# Create SDXL environment (newer libraries)\n",
        "!virtualenv /content/sdxl_env\n",
        "# Fix for activation script path\n",
        "!ls -la /content/sdxl_env/bin/\n",
        "\n",
        "# Install specific PyTorch version for Stable Diffusion\n",
        "!. /content/sdxl_env/bin/activate && pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124 -q\n",
        "# Install specific packages for SD environment\n",
        "!. /content/sdxl_env/bin/activate && pip install -r/content/requirements_sdxl.txt -q\n",
        "\n",
        "# Create OOTDiffusion environment (older libraries)\n",
        "!virtualenv /content/ootd_env\n",
        "\n",
        "# Install specific PyTorch version for OOTDiffusion\n",
        "!. /content/ootd_env/bin/activate && pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "# Install from your existing requirements file\n",
        "!. /content/ootd_env/bin/activate && pip install -r/content/requirements_ootd.txt -q"
      ],
      "metadata": {
        "id": "Z6rXaIoO-6QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!. /content/ootd_env/bin/activate && pip install git+https://github.com/huggingface/diffusers.git -q"
      ],
      "metadata": {
        "id": "XRmSqkzho6AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning OOTDiffusion and checkpoints"
      ],
      "metadata": {
        "id": "XqOcUMEuUYs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://github.com/levihsu/OOTDiffusion\n",
        "%cd /content/OOTDiffusion\n",
        "!git clone https://huggingface.co/openai/clip-vit-large-patch14 checkpoints/clip-vit-large-patch14\n",
        "!git clone https://huggingface.co/levihsu/OOTDiffusion\n",
        "\n",
        "# Move openpose,humanparsing and ootd\n",
        "!mv -n /content/OOTDiffusion/OOTDiffusion/checkpoints/ootd /content/OOTDiffusion/checkpoints\n",
        "!mv -n /content/OOTDiffusion/OOTDiffusion/checkpoints/humanparsing /content/OOTDiffusion/checkpoints\n",
        "!mv -n /content/OOTDiffusion/OOTDiffusion/checkpoints/openpose /content/OOTDiffusion/checkpoints\n",
        "%cd /content/OOTDiffusion/run"
      ],
      "metadata": {
        "id": "K7IGs56ukkPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*You need to go to /content/ootd_env/lib/python3.11/site-packages/diffusers/utils/dynamic_modules_utils.py and remove cached_download from the line \"from huggingface_hub import HfFolder, cached_download, hf_hub_download, model_info\""
      ],
      "metadata": {
        "id": "wqeac82QS2Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/generate_clothing.py\n",
        "def generate_clothing_image(\n",
        "    prompt,\n",
        "    lora_path=None,\n",
        "    negative_prompt=None,\n",
        "    num_steps=30,\n",
        "    guidance_scale=7.5,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a clothing image using Stable Diffusion 2.1 with optional LoRA weights.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt describing the clothing to generate\n",
        "        lora_path (str, optional): Path to LoRA adapter weights\n",
        "        lora_scale (float, optional): Scale factor for LoRA weights\n",
        "        negative_prompt (str, optional): Negative prompt to guide generation away from certain features\n",
        "        num_steps (int, optional): Number of inference steps\n",
        "        guidance_scale (float, optional): Guidance scale for generation\n",
        "        seed (int, optional): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: Generated image\n",
        "        str: Path where the image was saved\n",
        "    \"\"\"\n",
        "    from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "    import torch\n",
        "    from safetensors.torch import load_file\n",
        "\n",
        "    # Default negative prompt if none provided\n",
        "    if negative_prompt is None:\n",
        "        negative_prompt = \"wrinkled, dirty, worn, text, logo, brand name, person, model, mannequin, low quality, worst quality, blurry\"\n",
        "\n",
        "    # Initialize pipeline with SD 2.1\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-2-1-base\",\n",
        "        torch_dtype=torch.float16,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "\n",
        "    # Load and apply LoRA weights if provided\n",
        "    if lora_path:\n",
        "        # Method to load LoRA weights\n",
        "        pipe.unet.load_attn_procs(lora_path)\n",
        "\n",
        "\n",
        "    # Enable memory optimizations\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    # Enhance prompt with plain white background specification\n",
        "    enhanced_prompt = f\"{prompt}, on plain white background\"\n",
        "\n",
        "    image = pipe(\n",
        "        prompt=enhanced_prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=num_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "    ).images[0]\n",
        "\n",
        "    # Save image\n",
        "    save_path = \"/content/clothing.png\"\n",
        "    image.save(save_path)\n",
        "\n",
        "    return image, save_path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Generate clothing images with SD 2.1 and LoRA\")\n",
        "    parser.add_argument(\"--prompt\", type=str, required=True, help=\"Prompt for image generation\")\n",
        "    parser.add_argument(\"--lora_path\", type=str, help=\"Path to LoRA adapter weights\")\n",
        "    parser.add_argument(\"--negative_prompt\", type=str, help=\"Negative prompt\")\n",
        "    parser.add_argument(\"--steps\", type=int, default=30, help=\"Number of inference steps\")\n",
        "    parser.add_argument(\"--guidance\", type=float, default=7.5, help=\"Guidance scale\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    image, path = generate_clothing_image(\n",
        "        prompt=args.prompt,\n",
        "        lora_path=args.lora_path,\n",
        "        negative_prompt=args.negative_prompt,\n",
        "        num_steps=args.steps,\n",
        "        guidance_scale=args.guidance,\n",
        "        seed=args.seed\n",
        "    )\n",
        "\n",
        "    print(f\"Image saved to {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_eC7g4R-OaQ",
        "outputId": "2b3cfe90-1b82-409b-dc02-59fdf23c70b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/generate_clothing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating clothes and fitting model"
      ],
      "metadata": {
        "id": "iqm1zoVCTzje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!source /content/sdxl_env/bin/activate && python /content/generate_clothing.py --prompt \"simple grey tshirt\" \\\n",
        " --lora_path \"/content/pytorch_lora_weights.safetensors\" --steps 30 --guidance 7.5"
      ],
      "metadata": {
        "id": "lRmWsgNA-NeY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change model path and cloth path,\n",
        "#Garment category must be paired: 0 = upperbody; 1 = lowerbody; 2 = dress\n",
        "# Step 2: Use the generated image with OOTDiffusion\n",
        "!source /content/ootd_env/bin/activate && python run_ootd.py --model_path /content/vt_test.jpg --cloth_path /content/clothing.png --model_type dc --category 0 --scale 2.0 --sample 4"
      ],
      "metadata": {
        "id": "i_hGmWZjUSlI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}